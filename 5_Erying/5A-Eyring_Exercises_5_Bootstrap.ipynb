{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapping\n",
    "\n",
    "The confidence intervals calculated with the uncertain fit parameters determined using ```scipy.optimize.curve_fit``` or ```lmfit.fit``` are symmetrical. We get $\\pm$ some value as the standard deviation. This is a perfectly good was to estimate the confidence interval (all errors are estimated) but it hides a subtle truth. The confidence is not symmetrical. The data does not bnecesssarity have the same contribution to each parameter across the range of measuremnts and some regions may have more error than others, especially with non-linear curve fits.\n",
    "\n",
    "One way to estimate a confidence interval that reflects these regional effects in your data series is to use bootstrapping. It is a computationally expensive method where we randomly select data points from our data set and perfomr the curve fit hundreds, perhaps thousands of times. Then we values for the parameters that encompass the 68%, 95% and 99% percentiles ($1\\sigma$, $2\\sigma$, $3\\sigma$) or any arbitrary range we like. The median values and these upper and lower bounds will reflect a more accurate picture of estimated error, if we have enough data.\n",
    "\n",
    "This method is not very useful with small data sets as there may not be enough combinations such that percentiles are meaningful. Choosing the 95% percentile range from 6 values is not useful. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "### Install and load packages\n",
    "# \n",
    "# #!pip install uncertainties              # uncomment to install dependancy\n",
    "\n",
    "from scipy.optimize import curve_fit     # tool for curve fitting\n",
    "from scipy.stats import linregress     # tool for curve fitting\n",
    "import scipy                             # includes tools for data analysis\n",
    "import scipy.stats\n",
    "import numpy as np                       # import the tools of NumPy but use a shorter name, \"np\"\n",
    "from matplotlib import pyplot as plt     # tools for plotting\n",
    "import pandas as pd\n",
    "\n",
    "import uncertainties as un               # tool set for handling numbers with uncertainties\n",
    "from uncertainties import unumpy as unp  # a replacement for numpy that uses uncertainty values\n",
    "\n",
    "### Set global variables\n",
    "\n",
    "location_data = \"../data/\"                   ## Use either local folde or github folder. use github locations for Colab\n",
    "location_styles = \"../styles/\"\n",
    "#location_data = \"https://raw.githubusercontent.com/blinkletter/StealThisCode/main/data/\"\n",
    "#location_styles = \"https://raw.githubusercontent.com/blinkletter/StealThisCode/main/styles/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data Set\n",
    "\n",
    "We will begin with the 5-point data set that we have been using so far and then also use the 12-point data set from the csv data file as we have also seen done in this series of eamples. First, the smaller set will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00341297 0.0033557  0.00330033 0.00324675 0.00319489]\n"
     ]
    }
   ],
   "source": [
    "temp = [293, 298, 303, 308, 313]       # list of temperatures\n",
    "k_obs = [7.6, 11.7, 15.2, 21.3, 27.8]  # list of observe rate constants (s^-1)\n",
    "k_obs_err= [0.2 , 0.3, 0.1, 0.9, 0.9]  # list of standard deviations for data\n",
    "\n",
    "### Convert lists to numpy arrays (enables numpy math tools with these lists)\n",
    "temp = np.array(temp)\n",
    "k = unp.uarray(k_obs, k_obs_err)   # make an array of ufloat values\n",
    "\n",
    "x = 1/temp\n",
    "y_u = unp.log(k/temp)         # uncertain array for y-axis\n",
    "y = unp.nominal_values(y_u) # extract arrays of nominal values and errors\n",
    "y_err = unp.std_devs(y_u)   # because curve_fit can handle ufloats\n",
    "\n",
    "print(x)\n",
    "\n",
    "\n",
    "def linear(x, slope, intercept):\n",
    "    return slope * x + intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function bootstrap in module scipy.stats._resampling:\n",
      "\n",
      "bootstrap(data, statistic, *, n_resamples=9999, batch=None, vectorized=None, paired=False, axis=0, confidence_level=0.95, alternative='two-sided', method='BCa', bootstrap_result=None, random_state=None)\n",
      "    Compute a two-sided bootstrap confidence interval of a statistic.\n",
      "    \n",
      "    When `method` is ``'percentile'`` and `alternative` is ``'two-sided'``,\n",
      "    a bootstrap confidence interval is computed according to the following\n",
      "    procedure.\n",
      "    \n",
      "    1. Resample the data: for each sample in `data` and for each of\n",
      "       `n_resamples`, take a random sample of the original sample\n",
      "       (with replacement) of the same size as the original sample.\n",
      "    \n",
      "    2. Compute the bootstrap distribution of the statistic: for each set of\n",
      "       resamples, compute the test statistic.\n",
      "    \n",
      "    3. Determine the confidence interval: find the interval of the bootstrap\n",
      "       distribution that is\n",
      "    \n",
      "       - symmetric about the median and\n",
      "       - contains `confidence_level` of the resampled statistic values.\n",
      "    \n",
      "    While the ``'percentile'`` method is the most intuitive, it is rarely\n",
      "    used in practice. Two more common methods are available, ``'basic'``\n",
      "    ('reverse percentile') and ``'BCa'`` ('bias-corrected and accelerated');\n",
      "    they differ in how step 3 is performed.\n",
      "    \n",
      "    If the samples in `data` are  taken at random from their respective\n",
      "    distributions :math:`n` times, the confidence interval returned by\n",
      "    `bootstrap` will contain the true value of the statistic for those\n",
      "    distributions approximately `confidence_level`:math:`\\, \\times \\, n` times.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    data : sequence of array-like\n",
      "         Each element of data is a sample from an underlying distribution.\n",
      "    statistic : callable\n",
      "        Statistic for which the confidence interval is to be calculated.\n",
      "        `statistic` must be a callable that accepts ``len(data)`` samples\n",
      "        as separate arguments and returns the resulting statistic.\n",
      "        If `vectorized` is set ``True``,\n",
      "        `statistic` must also accept a keyword argument `axis` and be\n",
      "        vectorized to compute the statistic along the provided `axis`.\n",
      "    n_resamples : int, default: ``9999``\n",
      "        The number of resamples performed to form the bootstrap distribution\n",
      "        of the statistic.\n",
      "    batch : int, optional\n",
      "        The number of resamples to process in each vectorized call to\n",
      "        `statistic`. Memory usage is O( `batch` * ``n`` ), where ``n`` is the\n",
      "        sample size. Default is ``None``, in which case ``batch = n_resamples``\n",
      "        (or ``batch = max(n_resamples, n)`` for ``method='BCa'``).\n",
      "    vectorized : bool, optional\n",
      "        If `vectorized` is set ``False``, `statistic` will not be passed\n",
      "        keyword argument `axis` and is expected to calculate the statistic\n",
      "        only for 1D samples. If ``True``, `statistic` will be passed keyword\n",
      "        argument `axis` and is expected to calculate the statistic along `axis`\n",
      "        when passed an ND sample array. If ``None`` (default), `vectorized`\n",
      "        will be set ``True`` if ``axis`` is a parameter of `statistic`. Use of\n",
      "        a vectorized statistic typically reduces computation time.\n",
      "    paired : bool, default: ``False``\n",
      "        Whether the statistic treats corresponding elements of the samples\n",
      "        in `data` as paired.\n",
      "    axis : int, default: ``0``\n",
      "        The axis of the samples in `data` along which the `statistic` is\n",
      "        calculated.\n",
      "    confidence_level : float, default: ``0.95``\n",
      "        The confidence level of the confidence interval.\n",
      "    alternative : {'two-sided', 'less', 'greater'}, default: ``'two-sided'``\n",
      "        Choose ``'two-sided'`` (default) for a two-sided confidence interval,\n",
      "        ``'less'`` for a one-sided confidence interval with the lower bound\n",
      "        at ``-np.inf``, and ``'greater'`` for a one-sided confidence interval\n",
      "        with the upper bound at ``np.inf``. The other bound of the one-sided\n",
      "        confidence intervals is the same as that of a two-sided confidence\n",
      "        interval with `confidence_level` twice as far from 1.0; e.g. the upper\n",
      "        bound of a 95% ``'less'``  confidence interval is the same as the upper\n",
      "        bound of a 90% ``'two-sided'`` confidence interval.\n",
      "    method : {'percentile', 'basic', 'bca'}, default: ``'BCa'``\n",
      "        Whether to return the 'percentile' bootstrap confidence interval\n",
      "        (``'percentile'``), the 'basic' (AKA 'reverse') bootstrap confidence\n",
      "        interval (``'basic'``), or the bias-corrected and accelerated bootstrap\n",
      "        confidence interval (``'BCa'``).\n",
      "    bootstrap_result : BootstrapResult, optional\n",
      "        Provide the result object returned by a previous call to `bootstrap`\n",
      "        to include the previous bootstrap distribution in the new bootstrap\n",
      "        distribution. This can be used, for example, to change\n",
      "        `confidence_level`, change `method`, or see the effect of performing\n",
      "        additional resampling without repeating computations.\n",
      "    random_state : {None, int, `numpy.random.Generator`,\n",
      "                    `numpy.random.RandomState`}, optional\n",
      "    \n",
      "        Pseudorandom number generator state used to generate resamples.\n",
      "    \n",
      "        If `random_state` is ``None`` (or `np.random`), the\n",
      "        `numpy.random.RandomState` singleton is used.\n",
      "        If `random_state` is an int, a new ``RandomState`` instance is used,\n",
      "        seeded with `random_state`.\n",
      "        If `random_state` is already a ``Generator`` or ``RandomState``\n",
      "        instance then that instance is used.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    res : BootstrapResult\n",
      "        An object with attributes:\n",
      "    \n",
      "        confidence_interval : ConfidenceInterval\n",
      "            The bootstrap confidence interval as an instance of\n",
      "            `collections.namedtuple` with attributes `low` and `high`.\n",
      "        bootstrap_distribution : ndarray\n",
      "            The bootstrap distribution, that is, the value of `statistic` for\n",
      "            each resample. The last dimension corresponds with the resamples\n",
      "            (e.g. ``res.bootstrap_distribution.shape[-1] == n_resamples``).\n",
      "        standard_error : float or ndarray\n",
      "            The bootstrap standard error, that is, the sample standard\n",
      "            deviation of the bootstrap distribution.\n",
      "    \n",
      "    Warns\n",
      "    -----\n",
      "    `~scipy.stats.DegenerateDataWarning`\n",
      "        Generated when ``method='BCa'`` and the bootstrap distribution is\n",
      "        degenerate (e.g. all elements are identical).\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Elements of the confidence interval may be NaN for ``method='BCa'`` if\n",
      "    the bootstrap distribution is degenerate (e.g. all elements are identical).\n",
      "    In this case, consider using another `method` or inspecting `data` for\n",
      "    indications that other analysis may be more appropriate (e.g. all\n",
      "    observations are identical).\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] B. Efron and R. J. Tibshirani, An Introduction to the Bootstrap,\n",
      "       Chapman & Hall/CRC, Boca Raton, FL, USA (1993)\n",
      "    .. [2] Nathaniel E. Helwig, \"Bootstrap Confidence Intervals\",\n",
      "       http://users.stat.umn.edu/~helwig/notes/bootci-Notes.pdf\n",
      "    .. [3] Bootstrapping (statistics), Wikipedia,\n",
      "       https://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Suppose we have sampled data from an unknown distribution.\n",
      "    \n",
      "    >>> import numpy as np\n",
      "    >>> rng = np.random.default_rng()\n",
      "    >>> from scipy.stats import norm\n",
      "    >>> dist = norm(loc=2, scale=4)  # our \"unknown\" distribution\n",
      "    >>> data = dist.rvs(size=100, random_state=rng)\n",
      "    \n",
      "    We are interested in the standard deviation of the distribution.\n",
      "    \n",
      "    >>> std_true = dist.std()      # the true value of the statistic\n",
      "    >>> print(std_true)\n",
      "    4.0\n",
      "    >>> std_sample = np.std(data)  # the sample statistic\n",
      "    >>> print(std_sample)\n",
      "    3.9460644295563863\n",
      "    \n",
      "    The bootstrap is used to approximate the variability we would expect if we\n",
      "    were to repeatedly sample from the unknown distribution and calculate the\n",
      "    statistic of the sample each time. It does this by repeatedly resampling\n",
      "    values *from the original sample* with replacement and calculating the\n",
      "    statistic of each resample. This results in a \"bootstrap distribution\" of\n",
      "    the statistic.\n",
      "    \n",
      "    >>> import matplotlib.pyplot as plt\n",
      "    >>> from scipy.stats import bootstrap\n",
      "    >>> data = (data,)  # samples must be in a sequence\n",
      "    >>> res = bootstrap(data, np.std, confidence_level=0.9,\n",
      "    ...                 random_state=rng)\n",
      "    >>> fig, ax = plt.subplots()\n",
      "    >>> ax.hist(res.bootstrap_distribution, bins=25)\n",
      "    >>> ax.set_title('Bootstrap Distribution')\n",
      "    >>> ax.set_xlabel('statistic value')\n",
      "    >>> ax.set_ylabel('frequency')\n",
      "    >>> plt.show()\n",
      "    \n",
      "    The standard error quantifies this variability. It is calculated as the\n",
      "    standard deviation of the bootstrap distribution.\n",
      "    \n",
      "    >>> res.standard_error\n",
      "    0.24427002125829136\n",
      "    >>> res.standard_error == np.std(res.bootstrap_distribution, ddof=1)\n",
      "    True\n",
      "    \n",
      "    The bootstrap distribution of the statistic is often approximately normal\n",
      "    with scale equal to the standard error.\n",
      "    \n",
      "    >>> x = np.linspace(3, 5)\n",
      "    >>> pdf = norm.pdf(x, loc=std_sample, scale=res.standard_error)\n",
      "    >>> fig, ax = plt.subplots()\n",
      "    >>> ax.hist(res.bootstrap_distribution, bins=25, density=True)\n",
      "    >>> ax.plot(x, pdf)\n",
      "    >>> ax.set_title('Normal Approximation of the Bootstrap Distribution')\n",
      "    >>> ax.set_xlabel('statistic value')\n",
      "    >>> ax.set_ylabel('pdf')\n",
      "    >>> plt.show()\n",
      "    \n",
      "    This suggests that we could construct a 90% confidence interval on the\n",
      "    statistic based on quantiles of this normal distribution.\n",
      "    \n",
      "    >>> norm.interval(0.9, loc=std_sample, scale=res.standard_error)\n",
      "    (3.5442759991341726, 4.3478528599786)\n",
      "    \n",
      "    Due to central limit theorem, this normal approximation is accurate for a\n",
      "    variety of statistics and distributions underlying the samples; however,\n",
      "    the approximation is not reliable in all cases. Because `bootstrap` is\n",
      "    designed to work with arbitrary underlying distributions and statistics,\n",
      "    it uses more advanced techniques to generate an accurate confidence\n",
      "    interval.\n",
      "    \n",
      "    >>> print(res.confidence_interval)\n",
      "    ConfidenceInterval(low=3.57655333533867, high=4.382043696342881)\n",
      "    \n",
      "    If we sample from the original distribution 1000 times and form a bootstrap\n",
      "    confidence interval for each sample, the confidence interval\n",
      "    contains the true value of the statistic approximately 90% of the time.\n",
      "    \n",
      "    >>> n_trials = 1000\n",
      "    >>> ci_contains_true_std = 0\n",
      "    >>> for i in range(n_trials):\n",
      "    ...    data = (dist.rvs(size=100, random_state=rng),)\n",
      "    ...    ci = bootstrap(data, np.std, confidence_level=0.9, n_resamples=1000,\n",
      "    ...                   random_state=rng).confidence_interval\n",
      "    ...    if ci[0] < std_true < ci[1]:\n",
      "    ...        ci_contains_true_std += 1\n",
      "    >>> print(ci_contains_true_std)\n",
      "    875\n",
      "    \n",
      "    Rather than writing a loop, we can also determine the confidence intervals\n",
      "    for all 1000 samples at once.\n",
      "    \n",
      "    >>> data = (dist.rvs(size=(n_trials, 100), random_state=rng),)\n",
      "    >>> res = bootstrap(data, np.std, axis=-1, confidence_level=0.9,\n",
      "    ...                 n_resamples=1000, random_state=rng)\n",
      "    >>> ci_l, ci_u = res.confidence_interval\n",
      "    \n",
      "    Here, `ci_l` and `ci_u` contain the confidence interval for each of the\n",
      "    ``n_trials = 1000`` samples.\n",
      "    \n",
      "    >>> print(ci_l[995:])\n",
      "    [3.77729695 3.75090233 3.45829131 3.34078217 3.48072829]\n",
      "    >>> print(ci_u[995:])\n",
      "    [4.88316666 4.86924034 4.32032996 4.2822427  4.59360598]\n",
      "    \n",
      "    And again, approximately 90% contain the true value, ``std_true = 4``.\n",
      "    \n",
      "    >>> print(np.sum((ci_l < std_true) & (std_true < ci_u)))\n",
      "    900\n",
      "    \n",
      "    `bootstrap` can also be used to estimate confidence intervals of\n",
      "    multi-sample statistics, including those calculated by hypothesis\n",
      "    tests. `scipy.stats.mood` perform's Mood's test for equal scale parameters,\n",
      "    and it returns two outputs: a statistic, and a p-value. To get a\n",
      "    confidence interval for the test statistic, we first wrap\n",
      "    `scipy.stats.mood` in a function that accepts two sample arguments,\n",
      "    accepts an `axis` keyword argument, and returns only the statistic.\n",
      "    \n",
      "    >>> from scipy.stats import mood\n",
      "    >>> def my_statistic(sample1, sample2, axis):\n",
      "    ...     statistic, _ = mood(sample1, sample2, axis=-1)\n",
      "    ...     return statistic\n",
      "    \n",
      "    Here, we use the 'percentile' method with the default 95% confidence level.\n",
      "    \n",
      "    >>> sample1 = norm.rvs(scale=1, size=100, random_state=rng)\n",
      "    >>> sample2 = norm.rvs(scale=2, size=100, random_state=rng)\n",
      "    >>> data = (sample1, sample2)\n",
      "    >>> res = bootstrap(data, my_statistic, method='basic', random_state=rng)\n",
      "    >>> print(mood(sample1, sample2)[0])  # element 0 is the statistic\n",
      "    -5.521109549096542\n",
      "    >>> print(res.confidence_interval)\n",
      "    ConfidenceInterval(low=-7.255994487314675, high=-4.016202624747605)\n",
      "    \n",
      "    The bootstrap estimate of the standard error is also available.\n",
      "    \n",
      "    >>> print(res.standard_error)\n",
      "    0.8344963846318795\n",
      "    \n",
      "    Paired-sample statistics work, too. For example, consider the Pearson\n",
      "    correlation coefficient.\n",
      "    \n",
      "    >>> from scipy.stats import pearsonr\n",
      "    >>> n = 100\n",
      "    >>> x = np.linspace(0, 10, n)\n",
      "    >>> y = x + rng.uniform(size=n)\n",
      "    >>> print(pearsonr(x, y)[0])  # element 0 is the statistic\n",
      "    0.9962357936065914\n",
      "    \n",
      "    We wrap `pearsonr` so that it returns only the statistic.\n",
      "    \n",
      "    >>> def my_statistic(x, y):\n",
      "    ...     return pearsonr(x, y)[0]\n",
      "    \n",
      "    We call `bootstrap` using ``paired=True``.\n",
      "    Also, since ``my_statistic`` isn't vectorized to calculate the statistic\n",
      "    along a given axis, we pass in ``vectorized=False``.\n",
      "    \n",
      "    >>> res = bootstrap((x, y), my_statistic, vectorized=False, paired=True,\n",
      "    ...                 random_state=rng)\n",
      "    >>> print(res.confidence_interval)\n",
      "    ConfidenceInterval(low=0.9950085825848624, high=0.9971212407917498)\n",
      "    \n",
      "    The result object can be passed back into `bootstrap` to perform additional\n",
      "    resampling:\n",
      "    \n",
      "    >>> len(res.bootstrap_distribution)\n",
      "    9999\n",
      "    >>> res = bootstrap((x, y), my_statistic, vectorized=False, paired=True,\n",
      "    ...                 n_resamples=1001, random_state=rng,\n",
      "    ...                 bootstrap_result=res)\n",
      "    >>> len(res.bootstrap_distribution)\n",
      "    11000\n",
      "    \n",
      "    or to change the confidence interval options:\n",
      "    \n",
      "    >>> res2 = bootstrap((x, y), my_statistic, vectorized=False, paired=True,\n",
      "    ...                  n_resamples=0, random_state=rng, bootstrap_result=res,\n",
      "    ...                  method='percentile', confidence_level=0.9)\n",
      "    >>> np.testing.assert_equal(res2.bootstrap_distribution,\n",
      "    ...                         res.bootstrap_distribution)\n",
      "    >>> res.confidence_interval\n",
      "    ConfidenceInterval(low=0.9950035351407804, high=0.9971170323404578)\n",
      "    \n",
      "    without repeating computation of the original bootstrap distribution.\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "linear() missing 2 required positional arguments: 'm' and 'b'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     y \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m*\u001b[39mx \u001b[38;5;241m+\u001b[39m b\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n\u001b[0;32m----> 9\u001b[0m bootstrap(data\u001b[38;5;241m=\u001b[39m(x,y), statfunction \u001b[38;5;241m=\u001b[39m \u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpercentile\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: linear() missing 2 required positional arguments: 'm' and 'b'"
     ]
    }
   ],
   "source": [
    "import scikits.bootstrap as bs\n",
    "from scipy.stats import bootstrap\n",
    "\n",
    "m = 1; b = 1\n",
    "def linear(x, m, b):\n",
    "    y = m*x + b\n",
    "    return y\n",
    "\n",
    "bootstrap(data=(x,y), statfunction = linear((x,m,b)), method = \"percentile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.48918114, 0.59969109])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs.ci(np.random.rand(100), np.average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
